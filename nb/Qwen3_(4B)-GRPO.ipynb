{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danel-phang/Cat-Qwen3-4B/blob/main/nb/Qwen3_(4B)-GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93rahq3ADNer"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm==0.8.5.post1"
      ],
      "metadata": {
        "id": "mLNuxXDt0HgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth bitsandbytes accelerate xformers peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer blake3 fastapi"
      ],
      "metadata": {
        "id": "9Kvow-37yMq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,  # LoRA 方式微调\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKHzG797yNVI",
        "outputId": "650410b7-5726-4b62-f6fb-80c82e3e135e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,  # LoRA缩放系数\n",
        "    lora_dropout = 0.0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "sLkE-w3bysD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_ds = load_dataset(\n",
        "    \"json\",\n",
        "    data_files = {\"train\": \"cat.json\"},\n",
        "    split = \"train\"\n",
        ")\n",
        "# 将原始JSON转换为对话格式列表，便于后续模板化\n",
        "convs = []\n",
        "for item in raw_ds:\n",
        "    convs.append([\n",
        "        {\"role\": \"user\",      \"content\": item[\"instruction\"]},\n",
        "        {\"role\": \"assistant\", \"content\": item[\"output\"]},\n",
        "    ])"
      ],
      "metadata": {
        "id": "yu6foJYxysmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "# 将 list 转成 Dataset\n",
        "raw_conv_ds = Dataset.from_dict({\"conversations\": convs})\n",
        "\n",
        "standardized = standardize_sharegpt(raw_conv_ds)\n",
        "\n",
        "chat_inputs = tokenizer.apply_chat_template(\n",
        "    standardized[\"conversations\"],\n",
        "    tokenize = False,\n",
        ")"
      ],
      "metadata": {
        "id": "A3z4k8BFyueb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "df = pd.DataFrame({\"text\": chat_inputs})\n",
        "train_ds = Dataset.from_pandas(df).shuffle(seed = 666)"
      ],
      "metadata": {
        "id": "FnL-qtKIyv3G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_ds,\n",
        "    eval_dataset = None,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        max_steps = 666,          # 训练步数，调大一点，毕竟小模型微调起来挺快的\n",
        "        learning_rate = 2e-4,\n",
        "        warmup_steps = 10,\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 666,\n",
        "        report_to = \"none\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "kH1lsVkgyxOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 开始训练"
      ],
      "metadata": {
        "id": "GC2ykWdv0-XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n",
        "print(trainer_stats)"
      ],
      "metadata": {
        "id": "i3CwuEcXyymV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"Cat-Qwen3-4B\")  # 保存lora模型和分词器"
      ],
      "metadata": {
        "id": "sUue8fOF5kXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_catgirl(question):\n",
        "  messages = [\n",
        "    {\"role\" : \"user\", \"content\" : question}\n",
        "]\n",
        "  text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True,\n",
        "    enable_thinking = False, # 思考模式\n",
        ")\n",
        "\n",
        "  from transformers import TextStreamer\n",
        "  _ = model.generate(\n",
        "      **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "      max_new_tokens = 256, # 输出长度\n",
        "      temperature = 0.7, top_p = 0.8, top_k = 20,\n",
        "      streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "  )"
      ],
      "metadata": {
        "id": "4tSHnWTTy1Sz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_catgirl(\"你会什么\")"
      ],
      "metadata": {
        "id": "_T-Yr-q3y4py"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}